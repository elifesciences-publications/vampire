{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for doing a balanced 50/50 train/test splitting of a dataset using clustering.\n",
    "Specifically the procedure for making these clusters is:\n",
    "1. Split sequences into partitions based on the AHo positions they have gaps.\n",
    "2. Run clustering on the sequences inside each partition. DBSCAN clustering is used with parameters that makes it equal to single linkage clustering with the minimum distance between clusters indicated by the `eps` values in the filenames of the fasta files in this folder.\n",
    "3. Iterating through each partition the 50/50 split is made by sorting the clusters in the partition according to size. Iterating through clusters in a pairwise fashion e.g. (1, 2), (3, 4), ... (n-1, n), the larger of the pair is down-sampled to the size of the smaller and these are then assigned a random split and written to either the train or test split fasta file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports:\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import math, random, re\n",
    "import time\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "import multiprocessing\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amino acid alphabet:\n",
    "AA_ORDER = 'ACDEFGHIKLMNPQRSTVWY-'\n",
    "AA_LIST = list(AA_ORDER)\n",
    "AA_DICT = {c:i for i, c in enumerate(AA_LIST)}\n",
    "AA_DICT_REV = {i:c for i, c in enumerate(AA_LIST)}\n",
    "AA_SET = set(AA_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import jellyfish\n",
    "    def hamming_distance(s1, s2):\n",
    "        if s1 == s2:\n",
    "            return 0\n",
    "        else:\n",
    "            return jellyfish.hamming_distance(s1, s2)\n",
    "except:\n",
    "    def hamming_distance(seq1, seq2):\n",
    "        '''Hamming distance between two sequences of equal length'''\n",
    "        return sum(x != y for x, y in zip(seq1, seq2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pairwise_dists(x):\n",
    "    N = 0\n",
    "    dists = dict()\n",
    "    for i, si in enumerate(x):\n",
    "        for j, sj in enumerate(x[i:], start=i):\n",
    "            d = jellyfish.hamming_distance(si, sj)\n",
    "            if d <= DIST_CUT:\n",
    "                ij = tuple(sorted([i, j]))\n",
    "                dists[ij] = d\n",
    "                N += 1\n",
    "                assert(N <= MAX_DISTS)\n",
    "    return(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQS = 100000000  # Read no more than this number of sequences\n",
    "MAX_LEN = 149         # Length of sequences\n",
    "DIST_CUT = 15         # Only keep pairwise distance equal to or smaller than this\n",
    "MAX_DISTS = int(1e9)  # Abort if more than this number of distances are found for a partition\n",
    "MIN_SEQS_PER_PARTITION = 100\n",
    "\n",
    "seq_gap_dict_filename = 'seq_gap_dict.p'\n",
    "dist_dict_filename = 'dist_dict.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 1602878 sequences.\n",
      "This is how a typical sequence looks:\n",
      "EVQLVES-GGGLVQPGGSLRLSCAASG-FPSNS-----YWMTWVRQAPGKGLEWVANINED---GSERYYVDSVKGRFTISRDNAKNSQYLQMNSLRAEDTAVYYCTRDVWF---------------------GFFDIWGQGTTVIVSS\n",
      "Found 268 partitions.\n",
      "The largest partition has 126369 sequence.\n"
     ]
    }
   ],
   "source": [
    "# Read the sequences and partition into buckets according to their gap profile:\n",
    "if not os.path.isfile(seq_gap_dict_filename):\n",
    "    # Read in some sequences:\n",
    "    fnam = '../BCR_data/spurf_heavy_chain_AHo.fasta'\n",
    "    seq_list = list()\n",
    "    for i, record in enumerate(SeqIO.parse(fnam, 'fasta')):\n",
    "        if i >= MAX_SEQS:\n",
    "            break\n",
    "        seq = str(record.seq)\n",
    "        if len(seq) > MAX_LEN:\n",
    "            continue\n",
    "        else:\n",
    "            seq += '-' * (MAX_LEN - len(seq))\n",
    "            seq_list.append(seq)\n",
    "\n",
    "    print('Input data has {} sequences.'.format(len(seq_list)))\n",
    "    print('This is how a typical sequence looks:\\n{}'.format(seq_list[0]))\n",
    "\n",
    "    # Partition the sequences by gap positions:\n",
    "    seq_gap_dict = dict()\n",
    "    for seq in seq_list:\n",
    "        gap_key = tuple(i for i, nt in enumerate(seq) if nt == '-')\n",
    "        if gap_key not in seq_gap_dict:\n",
    "            seq_gap_dict[gap_key] = [seq]\n",
    "        else:\n",
    "            seq_gap_dict[gap_key].append(seq)\n",
    "    print('Found {} partitions.'.format(len(seq_gap_dict)))\n",
    "\n",
    "    # Sort the gap keys according to number of sequences in the partition:\n",
    "    gap_key_sorted = [ks[1] for ks in sorted([(len(l[1]), l[0]) for l in seq_gap_dict.items()], reverse=True, key=lambda x: x[0])]\n",
    "    print('The largest partition has {} sequence.'.format(len(seq_gap_dict[gap_key_sorted[0]])))\n",
    "\n",
    "    # Dump the results:\n",
    "    with open('seq_gap_dict.p', 'wb') as fho:\n",
    "        pickle.dump(seq_gap_dict, fho)\n",
    "else:\n",
    "    with open('seq_gap_dict.p', 'rb') as fh:\n",
    "        seq_gap_dict = pickle.load(fh)\n",
    "    gap_key_sorted = [ks[1] for ks in sorted([(len(l[1]), l[0]) for l in seq_gap_dict.items()], reverse=True, key=lambda x: x[0])]\n",
    "    print('The largest partition has {} sequence.'.format(len(seq_gap_dict[gap_key_sorted[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all pairwise distances within a bucket\n",
    "# and keep those that a smaller than or equal to DIST_CUT:\n",
    "if not os.path.isfile(dist_dict_filename):  # This takes several hours and ~140 gb memory\n",
    "    # Calculate pairwise distances:\n",
    "    dist_dict = dict()\n",
    "    for ks in gap_key_sorted:  # Start from the largest to the smallest\n",
    "        dist_dict[ks] = calculate_pairwise_dists(seq_gap_dict[ks])\n",
    "    # Dump results:\n",
    "    with open('dist_dict.p', 'wb') as fho:\n",
    "        pickle.dump(dist_dict, fho)\n",
    "else:\n",
    "    with open('dist_dict.p', 'rb') as fh:\n",
    "        dist_dict = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune seq_gap_dict and dist_dict to only contain partitions\n",
    "# with more than MIN_SEQS_PER_PARTITION sequences:\n",
    "if MIN_SEQS_PER_PARTITION > 0:\n",
    "    for ks in list(seq_gap_dict.keys()):\n",
    "        if len(seq_gap_dict[ks]) < MIN_SEQS_PER_PARTITION:\n",
    "            del seq_gap_dict[ks]\n",
    "            del dist_dict[ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering code:\n",
    "def find_neighbors(dist_dict, seq_gap_dict, eps):\n",
    "    neighbor_list_dict = dict()\n",
    "    for ks in dist_dict:\n",
    "        neighbor_list_dict[ks] = [set() for i in range(len(seq_gap_dict[ks]))]\n",
    "        for ij in dist_dict[ks]:\n",
    "            #print(ij)\n",
    "            #sys.exit()\n",
    "            if dist_dict[ks][ij] <= eps:\n",
    "                neighbor_list_dict[ks][ij[0]].add(ij[1])\n",
    "                neighbor_list_dict[ks][ij[1]].add(ij[0])\n",
    "    return(neighbor_list_dict)\n",
    "\n",
    "def expand_cluster(neighbor_list, minPts, core_neighbor, clusters, visited):\n",
    "    '''\n",
    "    Fully expand the newly created cluster based on the neighbor points\n",
    "    of the first added core point.\n",
    "    '''\n",
    "    core_list = list(core_neighbor)\n",
    "    # Keep looping until there are no more neighbors to the core points:\n",
    "    while core_list:\n",
    "        # Extract a neighbor to a core point:\n",
    "        point_num = core_list.pop()\n",
    "        # Skip points that are already part of a cluster:\n",
    "        if visited[point_num] == 1:\n",
    "            continue\n",
    "        else:\n",
    "            visited[point_num] = 1\n",
    "\n",
    "        # Make a table lookup to find points\n",
    "        # within 1 'eps' distance of the given point:\n",
    "        neighborPts = neighbor_list[point_num]\n",
    "        if len(neighborPts) >= minPts:  # Core point\n",
    "            # Find new points that should be added to the search: \n",
    "            new_points = list(neighborPts - core_neighbor.intersection(neighborPts))\n",
    "            # Update the core neighbor set to avoid searching\n",
    "            # the same points multiple times:\n",
    "            core_neighbor.update(new_points)\n",
    "            # Update the list looping over:\n",
    "            core_list.extend(new_points)\n",
    "            # Adding the core point:\n",
    "            clusters[-1][0].add(point_num)            \n",
    "        else:  # Border point:\n",
    "            clusters[-1][1].add(point_num)\n",
    "    return(clusters)\n",
    "\n",
    "def dbscan(neighbor_list, minPts):\n",
    "    '''\n",
    "    Run DBSCAN on a list of pre-computed neighbors defined by an eps distance.\n",
    "    '''\n",
    "    # One tuple per cluster containing two sets,\n",
    "    # one for core points and one for border points: \n",
    "    clusters = list()\n",
    "    npoints = len(neighbor_list)  # Total number of points\n",
    "    visited = np.zeros(npoints, dtype=np.int8)  # 0/1 switch to determine if a point has been visited\n",
    "    for point_num in range(npoints):\n",
    "        # Skip points already visited:\n",
    "        if visited[point_num] == 1:\n",
    "            continue\n",
    "        else:\n",
    "            visited[point_num] = 1\n",
    "\n",
    "        # Make a table lookup to find points\n",
    "        # within 1 'eps' distance of the given point:\n",
    "        neighborPts = neighbor_list[point_num]\n",
    "\n",
    "        # Check if the point has enough neighbors to start a new cluster:\n",
    "        if len(neighborPts) >= minPts:  # This is a core point\n",
    "            clusters.append(({point_num}, set()))  # Start of a new cluster\n",
    "            # Now fully expand this newly created cluster:\n",
    "            clusters = expand_cluster(neighbor_list, minPts, neighborPts, clusters, visited)\n",
    "\n",
    "    return(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the 50/50 train/test splits and down-sample to balance them:\n",
    "def train_test_split(clusters_eps):\n",
    "    train = dict()\n",
    "    test = dict()\n",
    "    for ks in clusters_eps:\n",
    "        train_ks = list()\n",
    "        test_ks = list()\n",
    "        N_clusters = len(clusters_eps[ks])\n",
    "        # There needs to be equal number of clusters in both train and test sets:\n",
    "        if N_clusters % 2 != 0:\n",
    "            N_clusters -= 1\n",
    "        assert(N_clusters % 2 == 0)\n",
    "        cl_i = 0\n",
    "        train_first = True\n",
    "        while cl_i < N_clusters:\n",
    "            if train_first:\n",
    "                train_first = False\n",
    "                train_clust = clusters_eps[ks][cl_i]\n",
    "                cl_i += 1\n",
    "                test_clust = clusters_eps[ks][cl_i]\n",
    "                cl_i += 1\n",
    "                # Downsample the train cluster to equal the test cluster:\n",
    "                train_clust = random.sample(train_clust, len(test_clust))\n",
    "            else:\n",
    "                train_first = True\n",
    "                test_clust = clusters_eps[ks][cl_i]\n",
    "                cl_i += 1\n",
    "                train_clust = clusters_eps[ks][cl_i]\n",
    "                cl_i += 1\n",
    "                # Downsample the test cluster to equal the train cluster:\n",
    "                test_clust = random.sample(test_clust, len(train_clust))\n",
    "            train_ks.extend(list(train_clust))\n",
    "            test_ks.extend(list(test_clust))\n",
    "        train[ks] = train_ks\n",
    "        test[ks] = test_ks\n",
    "    return(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the train/test splits:\n",
    "def dump_train_test_data(train, test, eps, seq_gap_dict, filename_prefix='data_split'):\n",
    "    train_printed = 0\n",
    "    test_printed = 0\n",
    "    fh_train = open('clusters/{}_trainset_eps{}.fasta'.format(filename_prefix, eps), 'w')\n",
    "    fh_test = open('clusters/{}_testset_eps{}.fasta'.format(filename_prefix, eps), 'w')\n",
    "    for ks in train:\n",
    "        train_idxs = set(train[ks])\n",
    "        test_idxs = set(test[ks])\n",
    "        assert(len(train_idxs.intersection(test_idxs)) == 0)\n",
    "        for idx, seq in enumerate(seq_gap_dict[ks]):\n",
    "            if idx in train_idxs:\n",
    "                ks_str = '-'.join(map(str, ks))\n",
    "                print('>{}_{}\\n{}'.format(ks_str, idx, seq), file=fh_train)\n",
    "                train_printed += 1\n",
    "            elif idx in test_idxs:\n",
    "                ks_str = '-'.join(map(str, ks))\n",
    "                print('>{}_{}\\n{}'.format(ks_str, idx, seq), file=fh_test)\n",
    "                test_printed += 1\n",
    "    fh_train.close()\n",
    "    fh_test.close()\n",
    "    assert(sum([len(train[ks]) for ks in train]) == train_printed)\n",
    "    assert(sum([len(test[ks]) for ks in test]) == test_printed)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make splits with different minimum distance (eps) between clusters:\n",
    "eps_list = list(range(1, DIST_CUT+1))\n",
    "minPts = 1\n",
    "for eps in eps_list:\n",
    "    clusters_eps = dict()\n",
    "    neighbor_key_list = find_neighbors(dist_dict, seq_gap_dict, eps)\n",
    "    for ks in neighbor_key_list:\n",
    "        neighbor_list = neighbor_key_list[ks]\n",
    "        clusters_eps[ks] = dbscan(neighbor_list, minPts)\n",
    "        # Sort according to cluster size,\n",
    "        # and discard the \"edge points\":\n",
    "        clusters_eps[ks] = [t[1] for t in sorted([(len(s[0]), s[0]) for s in clusters_eps[ks]], key=lambda x:x[0], reverse=True)]\n",
    "    # Split in train and test:\n",
    "    train, test = train_test_split(clusters_eps)\n",
    "    # Write the train/test data to files:\n",
    "    dump_train_test_data(train, test, eps, seq_gap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
